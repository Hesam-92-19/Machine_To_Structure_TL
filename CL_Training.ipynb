{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import shutil\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "import tqdm\n",
        "try:\n",
        "    from tqdm.notebook import tqdm\n",
        "except:\n",
        "    from tqdm import tqdm\n",
        "\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download complete!\n"
          ]
        }
      ],
      "source": [
        "# Download required files and set the directory\n",
        "cwd=os.getcwd()\n",
        "os.chdir(cwd)\n",
        "cwd+='/'\n",
        "url = 'https://www.dropbox.com/scl/fi/438cfug8ks6ye05zly4c4/RM_FS_TL.zip?rlkey=iiuadbrbeukf29t1goo7nyt75&st=2v0315ph&dl=1'\n",
        "destination = cwd+'Files.zip'\n",
        "import urllib\n",
        "#Some functions to handle downloads\n",
        "def download_file(url, destination):\n",
        "    urllib.request.urlretrieve(url, destination)\n",
        "\n",
        "def is_download_complete(url, destination):\n",
        "    # Get the size of the file from the Content-Length header\n",
        "    with urllib.request.urlopen(url) as response:\n",
        "        expected_size = int(response.headers['Content-Length'])\n",
        "\n",
        "    # Get the actual size of the downloaded file\n",
        "    actual_size = os.path.getsize(destination)\n",
        "\n",
        "    # Compare the expected size with the actual size\n",
        "    return expected_size == actual_size\n",
        "\n",
        "download_file(url, destination)\n",
        "\n",
        "if is_download_complete(url, destination):\n",
        "    print(\"Download complete!\")\n",
        "else:\n",
        "    print(\"Download Failed; Please retry\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Zip file extracted to c:\\Users\\solei\\Documents\\GitHub\\Machine_To_Structure_TL1/\n"
          ]
        }
      ],
      "source": [
        "# Building the file hierachy\n",
        "''' The file hierachy is as follows\n",
        "\n",
        "│── RM-FS-TL\n",
        "│       └── Models\n",
        "│           └── Blocks_geom*.parquet\n",
        "│           └── Blocks_geom*.parquet\n",
        "│           └── Blocks_geom*.parquet\n",
        "│           └── A_RM__S4_1000.npy (Rot. Machinery dataset \"A\", i.e., torque 0 Nm; 4-channel dataset)\n",
        "│           └── A_RM_Classes_1000.npy (Rot. Machinery dataset \"A\", i.e., torque 0 Nm; number of samples in each class)\n",
        "│           └── B_RM__S4_1000.npy (Rot. Machinery dataset \"B\", i.e., torque 2 Nm; 4-channel dataset)\n",
        "│           └── B_RM_Classes_1000.npy (Rot. Machinery dataset \"B\", i.e., torque 2 Nm; number of samples in each class)\n",
        "│           └── C_RM__S4_1000.npy (Rot. Machinery dataset \"C\", i.e., torque 4 Nm 4-channel dataset)\n",
        "│           └── C_RM_Classes_1000.npy (Rot. Machinery dataset \"C\", i.e., torque 4 Nm; number of samples in each class)\n",
        "│           └── Yellow_S4_1000.npy (Yellow Frame 4-channel dataset)\n",
        "│           └── Yellow_Classes_1000.npy (Yellow Frame 4-channel dataset; number of samples in each class)\n",
        "│           └── QUGS_S4_1000.npy (QUGS Frame 4-channel dataset)\n",
        "│           └── QUGS_Classes_1000.npy (QUGS 4-channel dataset; number of samples in each class)\n",
        "'''\n",
        "\n",
        "zip_file_path = cwd+'Files.zip'\n",
        "extract_dir = cwd\n",
        "\n",
        "shutil.unpack_archive(zip_file_path, extract_dir)\n",
        "print(f\"Zip file extracted to {extract_dir}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CL-Network\n",
        "W=1000\n",
        "class CustomNeuralNetworkS15(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        super(CustomNeuralNetworkS15, self).__init__()\n",
        "        Stacked_Layers=1\n",
        "        self.lstm1 = nn.LSTM(int(W/2), 100, Stacked_Layers, batch_first=True)\n",
        "        self.lstm2 = nn.LSTM(int(W/2), 100, Stacked_Layers, batch_first=True)\n",
        "        self.lstm3 = nn.LSTM(int(W/2), 100, Stacked_Layers, batch_first=True)\n",
        "        self.lstm4 = nn.LSTM(int(W/2), 100, Stacked_Layers, batch_first=True)\n",
        "        self.Flatten = nn.Flatten()\n",
        "        self.FC1=nn.Linear(400,100)\n",
        "        self.FC2=nn.Linear(100,1)\n",
        "        self.LR1 = nn.LeakyReLU(0.2)\n",
        "        self.LR2 = nn.LeakyReLU(0.2)\n",
        "        self.LR3 = nn.ReLU()\n",
        "        self.Sig=nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        out1,state = self.lstm1(x[:,:,0:int(W/2)])\n",
        "        out2,state = self.lstm2(x[:,:,int(W/2):2*int(W/2)])\n",
        "        out3,state = self.lstm3(x[:,:,2*int(W/2):3*int(W/2)])\n",
        "        out4,state = self.lstm4(x[:,:,3*int(W/2):4*int(W/2)])\n",
        "        LL=torch.cat((out1,out2,out3,out4),dim=2)\n",
        "        LLA=self.LR1(LL)\n",
        "        outCL=self.Flatten(LLA)\n",
        "        outNF=self.FC1(outCL)\n",
        "        LLB=self.LR1(outNF)        \n",
        "        outF=self.FC2(LLB)\n",
        "        outF=self.LR3(outF)\n",
        "        return outF\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "57fea14519a14c0694d56b19abe1ca4a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "for RM datasets A, B, and C:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min RM class samples:\t 1536\n",
            "RM data case length is thus taken as:\t 1500\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "dbb381dad42c424285186ea59f25b77a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "RM A class-by-class training:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min RM class samples:\t 1536\n",
            "RM data case length is thus taken as:\t 1500\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e5f743a9d4924aea94b3e6b6ab201f88",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "RM B class-by-class training:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min RM class samples:\t 1536\n",
            "RM data case length is thus taken as:\t 1500\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "97b1db09613949d6b3b71e2e875dd8af",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "RM C class-by-class training:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "'''\n",
        "As outlined earlier, there exists two folder named PT and NT inside the Models directory. \n",
        "The PT folder stores the pre-trained CL models, from which you may reconstruct the paper results. \n",
        "Additionally, you may train new CL models which will be stored inside the \"NT\" folder.\n",
        " You may later view results for the new models you trained also.\n",
        "'''\n",
        "# Three RM Dataset exists, Torque 0 Nm: 'A', Torque 0 Nm: 'B', Torque 0 Nm: 'C'\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "S=4\n",
        "for RM_N in tqdm(['A','B','C'],desc='for RM datasets A, B, and C'):\n",
        "    # Rotor data: Class 0: no damage\n",
        "    Rot_Data=np.load(cwd+'RM_FS_TL/'+RM_N+'_RM_S4_1000.npy')\n",
        "    # Samples in each Rotor data case:\n",
        "    Class_N=np.cumsum(np.load(cwd+'RM_FS_TL/'+RM_N+'_RM_Classes_1000.npy'))\n",
        "    # Minimum is\n",
        "    print('Min RM class samples:\\t',np.min(np.load(cwd+'RM_FS_TL/'+RM_N+'_RM_Classes_1000.npy')))\n",
        "    # we thus take \n",
        "    Class_L=int(np.floor(np.min(np.load(cwd+'RM_FS_TL/'+RM_N+'_RM_Classes_1000.npy'))/100))*100\n",
        "    print('RM data case length is thus taken as:\\t',Class_L)\n",
        "    # The above process was to make sure we have the same data for the no-damage and damage cases for the training\n",
        "    \n",
        "    # Training: As outline in the paper, we only train for the first 9 damage classes\n",
        "    # RM dataset damage classes\n",
        "    for tt in tqdm(range(9),desc='RM '+RM_N+' class-by-class training'):\n",
        "        DD1=Class_L-100\n",
        "        N=torch.tensor(Rot_Data[0:Class_L-100,:],device='cuda').reshape((Class_L-100,1,2000)).float()\n",
        "        D=torch.tensor(Rot_Data[int(Class_N[tt]):Class_N[tt]+DD1,:],device='cuda').reshape((DD1,1,2000)).float()\n",
        "\n",
        "        ## Validation Data\n",
        "        NV=torch.tensor(Rot_Data[Class_L-100:Class_L,:],device='cuda').reshape((100,1,2000)).float()\n",
        "        DV=torch.tensor(Rot_Data[Class_N[tt]+DD1:Class_N[tt]+DD1+100,:],device='cuda').reshape((100,1,2000)).float()\n",
        "        \n",
        "        # The anchored scores of 0 and 80 in the \\theta domain (refer to the article)\n",
        "        S_1=80\n",
        "        S_0=0\n",
        "        Y=torch.tensor(np.zeros((Class_L-100,1)), device='cuda').float()\n",
        "        Y[:,0]=S_0\n",
        "        Y1=torch.tensor(np.zeros((Class_L-100,1)), device='cuda').float()\n",
        "        Y1[:,0]=S_1\n",
        "    \n",
        "    \n",
        "        YV=torch.tensor(np.zeros((100,1)), device='cuda').float()\n",
        "        YV[:,0]=S_0\n",
        "        YV1=torch.tensor(np.zeros((100,1)), device='cuda').float()\n",
        "        YV1[:,0]=S_1\n",
        "    \n",
        "        X=torch.cat((N,D),dim=0)\n",
        "        X_v=torch.cat((NV,DV),dim=0)\n",
        "        y=torch.cat((Y,Y1),dim=0)\n",
        "        yv=torch.cat((YV,YV1),dim=0)\n",
        "        dataset = TensorDataset(X, y)\n",
        "        data_loader = DataLoader(dataset, 200, shuffle=True)\n",
        "        \n",
        "    ########### Training loop\n",
        "        Net=CustomNeuralNetworkS15().to(device=device)\n",
        "        Net.train()\n",
        "        epoch=1000\n",
        "        crit=torch.nn.functional.mse_loss\n",
        "        L=[]\n",
        "        lam=0\n",
        "        D_solver = torch.optim.Adam(Net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
        "        pr=1\n",
        "        zet=0\n",
        "        L_V=[1000]\n",
        "        L_T=[]\n",
        "        pp=0\n",
        "        for i in range(epoch):\n",
        "            for x_t,y_t in data_loader:\n",
        "                pred=Net.forward(x_t)\n",
        "                loss=crit(pred,y_t) \n",
        "                D_solver.zero_grad()\n",
        "                loss.backward()\n",
        "                D_solver.step()\n",
        "                if zet==0:\n",
        "                    L_T.append(loss.item())\n",
        "\n",
        "                # To automate the whole process in the case of the training loss divergence:\n",
        "                if loss.item()>max(L_T):\n",
        "                    Net=CustomNeuralNetworkS15().to(device=device)\n",
        "                    Net.train()  \n",
        "                    D_solver = torch.optim.Adam(Net.parameters(), lr=5e-4, betas=(0.9, 0.999))\n",
        "                    zet=0\n",
        "                    L_T=[]\n",
        "                L_T.append(loss.item())\n",
        "\n",
        "            # Validation loss    \n",
        "            predA=Net.forward(X_v)\n",
        "            lossA=crit(predA,yv) \n",
        "\n",
        "            # We train for a definite loop and ultimately, the model offering the least\n",
        "            # validation loss i staken\n",
        "            if lossA.item()<=np.min(L_V):\n",
        "                torch.save(Net,cwd+'RM_FS_TL/Models/NT/Model'+RM_N+str(tt+1)+'_W_1000.pth')\n",
        "                pp=pp+1\n",
        "            L_V.append(lossA.item())\n",
        "            zet+=1"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
